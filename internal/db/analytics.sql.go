// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.30.0
// source: analytics.sql

package db

import (
	"context"

	"github.com/jackc/pgx/v5/pgtype"
)

const getAdminStorageBreakdown = `-- name: GetAdminStorageBreakdown :many
SELECT
    CASE
        WHEN content_type LIKE 'image/%' THEN 'image'
        WHEN content_type LIKE 'video/%' THEN 'video'
        WHEN content_type LIKE 'audio/%' THEN 'audio'
        WHEN content_type = 'application/pdf' THEN 'pdf'
        ELSE 'other'
    END as file_type,
    COUNT(*) as file_count,
    COALESCE(SUM(size_bytes), 0)::bigint as total_bytes
FROM files
WHERE deleted_at IS NULL
GROUP BY 1
ORDER BY total_bytes DESC
`

type GetAdminStorageBreakdownRow struct {
	FileType   string `json:"file_type"`
	FileCount  int64  `json:"file_count"`
	TotalBytes int64  `json:"total_bytes"`
}

func (q *Queries) GetAdminStorageBreakdown(ctx context.Context) ([]GetAdminStorageBreakdownRow, error) {
	rows, err := q.db.Query(ctx, getAdminStorageBreakdown)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetAdminStorageBreakdownRow
	for rows.Next() {
		var i GetAdminStorageBreakdownRow
		if err := rows.Scan(&i.FileType, &i.FileCount, &i.TotalBytes); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getBandwidthStats = `-- name: GetBandwidthStats :one
SELECT
    COALESCE(SUM(fs.download_count), 0)::bigint as total_downloads,
    COALESCE(SUM(fs.download_count * f.size_bytes), 0)::bigint as estimated_bandwidth_bytes
FROM file_shares fs
JOIN files f ON f.id = fs.file_id
WHERE fs.created_at >= $1 AND f.deleted_at IS NULL
`

type GetBandwidthStatsRow struct {
	TotalDownloads          int64 `json:"total_downloads"`
	EstimatedBandwidthBytes int64 `json:"estimated_bandwidth_bytes"`
}

// Get CDN/download bandwidth statistics (approximation based on share downloads)
func (q *Queries) GetBandwidthStats(ctx context.Context, createdAt pgtype.Timestamptz) (GetBandwidthStatsRow, error) {
	row := q.db.QueryRow(ctx, getBandwidthStats, createdAt)
	var i GetBandwidthStatsRow
	err := row.Scan(&i.TotalDownloads, &i.EstimatedBandwidthBytes)
	return i, err
}

const getBandwidthStatsByUser = `-- name: GetBandwidthStatsByUser :one
SELECT
    COALESCE(SUM(fs.download_count), 0)::bigint as total_downloads,
    COALESCE(SUM(fs.download_count * f.size_bytes), 0)::bigint as estimated_bandwidth_bytes
FROM file_shares fs
JOIN files f ON f.id = fs.file_id
WHERE f.user_id = $1 AND fs.created_at >= $2 AND f.deleted_at IS NULL
`

type GetBandwidthStatsByUserParams struct {
	UserID    pgtype.UUID        `json:"user_id"`
	CreatedAt pgtype.Timestamptz `json:"created_at"`
}

type GetBandwidthStatsByUserRow struct {
	TotalDownloads          int64 `json:"total_downloads"`
	EstimatedBandwidthBytes int64 `json:"estimated_bandwidth_bytes"`
}

// Get bandwidth statistics for a specific user
func (q *Queries) GetBandwidthStatsByUser(ctx context.Context, arg GetBandwidthStatsByUserParams) (GetBandwidthStatsByUserRow, error) {
	row := q.db.QueryRow(ctx, getBandwidthStatsByUser, arg.UserID, arg.CreatedAt)
	var i GetBandwidthStatsByUserRow
	err := row.Scan(&i.TotalDownloads, &i.EstimatedBandwidthBytes)
	return i, err
}

const getCostForecast = `-- name: GetCostForecast :one
SELECT
    -- Storage costs
    COALESCE(SUM(f.size_bytes), 0)::bigint as total_storage_bytes,
    COUNT(DISTINCT f.id)::bigint as total_files,

    -- Processing costs (last 30 days extrapolated)
    (SELECT COUNT(*) FROM processing_jobs WHERE created_at >= NOW() - INTERVAL '30 days' AND status = 'completed')::bigint as jobs_last_30_days,
    (SELECT COUNT(*) FROM processing_jobs WHERE created_at >= NOW() - INTERVAL '30 days' AND status = 'completed' AND job_type IN ('video_transcode', 'video_hls'))::bigint as video_jobs_last_30_days,
    (SELECT COALESCE(SUM(EXTRACT(EPOCH FROM (completed_at - started_at))), 0) FROM processing_jobs WHERE created_at >= NOW() - INTERVAL '30 days' AND status = 'completed' AND job_type IN ('video_transcode', 'video_hls'))::bigint as video_processing_seconds_30_days,

    -- Bandwidth (share downloads)
    (SELECT COALESCE(SUM(download_count), 0) FROM file_shares WHERE created_at >= NOW() - INTERVAL '30 days')::bigint as downloads_30_days
FROM files f
WHERE f.deleted_at IS NULL
`

type GetCostForecastRow struct {
	TotalStorageBytes            int64 `json:"total_storage_bytes"`
	TotalFiles                   int64 `json:"total_files"`
	JobsLast30Days               int64 `json:"jobs_last_30_days"`
	VideoJobsLast30Days          int64 `json:"video_jobs_last_30_days"`
	VideoProcessingSeconds30Days int64 `json:"video_processing_seconds_30_days"`
	Downloads30Days              int64 `json:"downloads_30_days"`
}

// Get data for cost forecasting (storage, processing, bandwidth)
func (q *Queries) GetCostForecast(ctx context.Context) (GetCostForecastRow, error) {
	row := q.db.QueryRow(ctx, getCostForecast)
	var i GetCostForecastRow
	err := row.Scan(
		&i.TotalStorageBytes,
		&i.TotalFiles,
		&i.JobsLast30Days,
		&i.VideoJobsLast30Days,
		&i.VideoProcessingSeconds30Days,
		&i.Downloads30Days,
	)
	return i, err
}

const getDailyUsage = `-- name: GetDailyUsage :many
SELECT 
    dates.date::date as date,
    COALESCE(file_counts.uploads, 0)::bigint as uploads,
    COALESCE(job_counts.transforms, 0)::bigint as transforms
FROM (
    SELECT generate_series($2::date, CURRENT_DATE, '1 day'::interval)::date as date
) dates
LEFT JOIN (
    SELECT DATE(f2.created_at) as day, COUNT(*) as uploads
    FROM files f2
    WHERE f2.user_id = $1 AND f2.created_at >= $2 AND f2.deleted_at IS NULL
    GROUP BY DATE(f2.created_at)
) file_counts ON dates.date = file_counts.day
LEFT JOIN (
    SELECT DATE(pj.created_at) as day, COUNT(*) as transforms
    FROM processing_jobs pj
    JOIN files f3 ON f3.id = pj.file_id
    WHERE f3.user_id = $1 AND pj.created_at >= $2 AND f3.deleted_at IS NULL
    GROUP BY DATE(pj.created_at)
) job_counts ON dates.date = job_counts.day
ORDER BY dates.date
`

type GetDailyUsageParams struct {
	UserID  pgtype.UUID `json:"user_id"`
	Column2 pgtype.Date `json:"column_2"`
}

type GetDailyUsageRow struct {
	Date       pgtype.Date `json:"date"`
	Uploads    int64       `json:"uploads"`
	Transforms int64       `json:"transforms"`
}

func (q *Queries) GetDailyUsage(ctx context.Context, arg GetDailyUsageParams) ([]GetDailyUsageRow, error) {
	rows, err := q.db.Query(ctx, getDailyUsage, arg.UserID, arg.Column2)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetDailyUsageRow
	for rows.Next() {
		var i GetDailyUsageRow
		if err := rows.Scan(&i.Date, &i.Uploads, &i.Transforms); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getLargestFiles = `-- name: GetLargestFiles :many
SELECT id, filename, content_type, size_bytes, created_at
FROM files
WHERE user_id = $1 AND deleted_at IS NULL
ORDER BY size_bytes DESC
LIMIT $2
`

type GetLargestFilesParams struct {
	UserID pgtype.UUID `json:"user_id"`
	Limit  int32       `json:"limit"`
}

type GetLargestFilesRow struct {
	ID          pgtype.UUID        `json:"id"`
	Filename    string             `json:"filename"`
	ContentType string             `json:"content_type"`
	SizeBytes   int64              `json:"size_bytes"`
	CreatedAt   pgtype.Timestamptz `json:"created_at"`
}

func (q *Queries) GetLargestFiles(ctx context.Context, arg GetLargestFilesParams) ([]GetLargestFilesRow, error) {
	rows, err := q.db.Query(ctx, getLargestFiles, arg.UserID, arg.Limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetLargestFilesRow
	for rows.Next() {
		var i GetLargestFilesRow
		if err := rows.Scan(
			&i.ID,
			&i.Filename,
			&i.ContentType,
			&i.SizeBytes,
			&i.CreatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getProcessingVolumeByTier = `-- name: GetProcessingVolumeByTier :many
SELECT
    u.subscription_tier::text as tier,
    pj.job_type::text as job_type,
    COUNT(pj.id)::bigint as count,
    COALESCE(SUM(EXTRACT(EPOCH FROM (pj.completed_at - pj.started_at)))::bigint, 0) as total_duration_seconds
FROM processing_jobs pj
JOIN files f ON f.id = pj.file_id
JOIN users u ON u.id = f.user_id
WHERE pj.created_at >= $1 AND pj.status = 'completed' AND f.deleted_at IS NULL
GROUP BY u.subscription_tier, pj.job_type
ORDER BY tier, count DESC
`

type GetProcessingVolumeByTierRow struct {
	Tier                 string      `json:"tier"`
	JobType              string      `json:"job_type"`
	Count                int64       `json:"count"`
	TotalDurationSeconds interface{} `json:"total_duration_seconds"`
}

// Get processing volume grouped by subscription tier
func (q *Queries) GetProcessingVolumeByTier(ctx context.Context, createdAt pgtype.Timestamptz) ([]GetProcessingVolumeByTierRow, error) {
	rows, err := q.db.Query(ctx, getProcessingVolumeByTier, createdAt)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetProcessingVolumeByTierRow
	for rows.Next() {
		var i GetProcessingVolumeByTierRow
		if err := rows.Scan(
			&i.Tier,
			&i.JobType,
			&i.Count,
			&i.TotalDurationSeconds,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getProcessingVolumeByTypeOverTime = `-- name: GetProcessingVolumeByTypeOverTime :many

SELECT
    dates.date::date as date,
    pj.job_type::text as job_type,
    COUNT(pj.id)::bigint as count,
    COALESCE(SUM(EXTRACT(EPOCH FROM (pj.completed_at - pj.started_at)))::bigint, 0) as total_duration_seconds
FROM (
    SELECT generate_series($1::date, CURRENT_DATE, '1 day'::interval)::date as date
) dates
LEFT JOIN processing_jobs pj ON DATE(pj.created_at) = dates.date
    AND pj.status = 'completed'
GROUP BY dates.date, pj.job_type
ORDER BY dates.date, pj.job_type
`

type GetProcessingVolumeByTypeOverTimeRow struct {
	Date                 pgtype.Date `json:"date"`
	JobType              string      `json:"job_type"`
	Count                int64       `json:"count"`
	TotalDurationSeconds interface{} `json:"total_duration_seconds"`
}

// ============================================================================
// ENHANCED ANALYTICS - Processing Volume & Trends
// ============================================================================
// Get processing volume by job type over the last N days
func (q *Queries) GetProcessingVolumeByTypeOverTime(ctx context.Context, dollar_1 pgtype.Date) ([]GetProcessingVolumeByTypeOverTimeRow, error) {
	rows, err := q.db.Query(ctx, getProcessingVolumeByTypeOverTime, dollar_1)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetProcessingVolumeByTypeOverTimeRow
	for rows.Next() {
		var i GetProcessingVolumeByTypeOverTimeRow
		if err := rows.Scan(
			&i.Date,
			&i.JobType,
			&i.Count,
			&i.TotalDurationSeconds,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getRecentActivity = `-- name: GetRecentActivity :many
SELECT id, type, message, status, created_at FROM (
    SELECT 
        f.id::text as id,
        'upload' as type,
        f.filename || ' uploaded' as message,
        'success' as status,
        f.created_at as created_at
    FROM files f
    WHERE f.user_id = $1 AND f.deleted_at IS NULL
    
    UNION ALL
    
    SELECT 
        pj.id::text as id,
        'transform' as type,
        f.filename || ' ' || pj.job_type::text || 
            CASE pj.status 
                WHEN 'completed' THEN ' completed'
                WHEN 'failed' THEN ' failed'
                WHEN 'running' THEN ' processing'
                ELSE ' queued'
            END as message,
        CASE pj.status
            WHEN 'completed' THEN 'success'
            WHEN 'failed' THEN 'error'
            ELSE 'warning'
        END as status,
        COALESCE(pj.completed_at, pj.created_at) as created_at
    FROM processing_jobs pj
    JOIN files f ON f.id = pj.file_id
    WHERE f.user_id = $1 AND f.deleted_at IS NULL
    
    UNION ALL
    
    SELECT 
        b.id::text as id,
        'batch' as type,
        'Batch ' || LEFT(b.id::text, 8) || 
            CASE b.status
                WHEN 'completed' THEN ' completed (' || b.completed_files || ' files)'
                WHEN 'failed' THEN ' failed'
                WHEN 'partial' THEN ' partially completed'
                ELSE ' processing'
            END as message,
        CASE b.status
            WHEN 'completed' THEN 'success'
            WHEN 'failed' THEN 'error'
            ELSE 'warning'
        END as status,
        COALESCE(b.completed_at, b.created_at) as created_at
    FROM batch_operations b
    WHERE b.user_id = $1
) activity
ORDER BY created_at DESC
LIMIT 20
`

type GetRecentActivityRow struct {
	ID        string             `json:"id"`
	Type      string             `json:"type"`
	Message   interface{}        `json:"message"`
	Status    string             `json:"status"`
	CreatedAt pgtype.Timestamptz `json:"created_at"`
}

func (q *Queries) GetRecentActivity(ctx context.Context, userID pgtype.UUID) ([]GetRecentActivityRow, error) {
	rows, err := q.db.Query(ctx, getRecentActivity, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetRecentActivityRow
	for rows.Next() {
		var i GetRecentActivityRow
		if err := rows.Scan(
			&i.ID,
			&i.Type,
			&i.Message,
			&i.Status,
			&i.CreatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getStorageBreakdownByType = `-- name: GetStorageBreakdownByType :many
SELECT
    CASE
        WHEN content_type LIKE 'image/%' THEN 'image'
        WHEN content_type LIKE 'video/%' THEN 'video'
        WHEN content_type LIKE 'audio/%' THEN 'audio'
        WHEN content_type = 'application/pdf' THEN 'pdf'
        ELSE 'other'
    END as file_type,
    COUNT(*) as file_count,
    COALESCE(SUM(size_bytes), 0)::bigint as total_bytes
FROM files
WHERE user_id = $1 AND deleted_at IS NULL
GROUP BY 1
ORDER BY total_bytes DESC
`

type GetStorageBreakdownByTypeRow struct {
	FileType   string `json:"file_type"`
	FileCount  int64  `json:"file_count"`
	TotalBytes int64  `json:"total_bytes"`
}

func (q *Queries) GetStorageBreakdownByType(ctx context.Context, userID pgtype.UUID) ([]GetStorageBreakdownByTypeRow, error) {
	rows, err := q.db.Query(ctx, getStorageBreakdownByType, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetStorageBreakdownByTypeRow
	for rows.Next() {
		var i GetStorageBreakdownByTypeRow
		if err := rows.Scan(&i.FileType, &i.FileCount, &i.TotalBytes); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getStorageBreakdownByVariant = `-- name: GetStorageBreakdownByVariant :many
SELECT
    variant_type::text,
    COUNT(*) as variant_count,
    COALESCE(SUM(size_bytes), 0)::bigint as total_bytes
FROM file_variants fv
JOIN files f ON f.id = fv.file_id
WHERE f.user_id = $1 AND f.deleted_at IS NULL
GROUP BY 1
ORDER BY total_bytes DESC
`

type GetStorageBreakdownByVariantRow struct {
	VariantType  string `json:"variant_type"`
	VariantCount int64  `json:"variant_count"`
	TotalBytes   int64  `json:"total_bytes"`
}

func (q *Queries) GetStorageBreakdownByVariant(ctx context.Context, userID pgtype.UUID) ([]GetStorageBreakdownByVariantRow, error) {
	rows, err := q.db.Query(ctx, getStorageBreakdownByVariant, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetStorageBreakdownByVariantRow
	for rows.Next() {
		var i GetStorageBreakdownByVariantRow
		if err := rows.Scan(&i.VariantType, &i.VariantCount, &i.TotalBytes); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getStorageGrowthTrend = `-- name: GetStorageGrowthTrend :many
SELECT
    dates.date::date as date,
    COALESCE(SUM(daily_added.bytes_added) OVER (ORDER BY dates.date), 0)::bigint as cumulative_bytes,
    COALESCE(daily_added.bytes_added, 0)::bigint as bytes_added,
    COALESCE(daily_added.files_added, 0)::bigint as files_added
FROM (
    SELECT generate_series($1::date, CURRENT_DATE, '1 day'::interval)::date as date
) dates
LEFT JOIN (
    SELECT
        DATE(created_at) as day,
        SUM(size_bytes) as bytes_added,
        COUNT(*) as files_added
    FROM files
    WHERE created_at >= $1 AND deleted_at IS NULL
    GROUP BY DATE(created_at)
) daily_added ON dates.date = daily_added.day
ORDER BY dates.date
`

type GetStorageGrowthTrendRow struct {
	Date            pgtype.Date `json:"date"`
	CumulativeBytes int64       `json:"cumulative_bytes"`
	BytesAdded      int64       `json:"bytes_added"`
	FilesAdded      int64       `json:"files_added"`
}

// Get storage growth over time (cumulative by day)
func (q *Queries) GetStorageGrowthTrend(ctx context.Context, dollar_1 pgtype.Date) ([]GetStorageGrowthTrendRow, error) {
	rows, err := q.db.Query(ctx, getStorageGrowthTrend, dollar_1)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetStorageGrowthTrendRow
	for rows.Next() {
		var i GetStorageGrowthTrendRow
		if err := rows.Scan(
			&i.Date,
			&i.CumulativeBytes,
			&i.BytesAdded,
			&i.FilesAdded,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getTopFilesByTransforms = `-- name: GetTopFilesByTransforms :many
SELECT 
    f.id as file_id,
    f.filename,
    COUNT(pj.id)::bigint as transforms
FROM files f
JOIN processing_jobs pj ON pj.file_id = f.id
WHERE f.user_id = $1 
    AND pj.created_at >= DATE_TRUNC('month', NOW())
    AND f.deleted_at IS NULL
GROUP BY f.id, f.filename
ORDER BY transforms DESC
LIMIT 10
`

type GetTopFilesByTransformsRow struct {
	FileID     pgtype.UUID `json:"file_id"`
	Filename   string      `json:"filename"`
	Transforms int64       `json:"transforms"`
}

func (q *Queries) GetTopFilesByTransforms(ctx context.Context, userID pgtype.UUID) ([]GetTopFilesByTransformsRow, error) {
	rows, err := q.db.Query(ctx, getTopFilesByTransforms, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetTopFilesByTransformsRow
	for rows.Next() {
		var i GetTopFilesByTransformsRow
		if err := rows.Scan(&i.FileID, &i.Filename, &i.Transforms); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getTotalStorageByUser = `-- name: GetTotalStorageByUser :one
SELECT COALESCE(SUM(size_bytes), 0)::bigint as total_bytes
FROM files
WHERE user_id = $1 AND deleted_at IS NULL
`

func (q *Queries) GetTotalStorageByUser(ctx context.Context, userID pgtype.UUID) (int64, error) {
	row := q.db.QueryRow(ctx, getTotalStorageByUser, userID)
	var total_bytes int64
	err := row.Scan(&total_bytes)
	return total_bytes, err
}

const getTransformBreakdown = `-- name: GetTransformBreakdown :many
SELECT 
    pj.job_type::text as type,
    COUNT(*)::bigint as count
FROM processing_jobs pj
JOIN files f ON f.id = pj.file_id
WHERE f.user_id = $1 
    AND pj.created_at >= DATE_TRUNC('month', NOW())
    AND pj.status = 'completed'
    AND f.deleted_at IS NULL
GROUP BY pj.job_type
ORDER BY count DESC
`

type GetTransformBreakdownRow struct {
	Type  string `json:"type"`
	Count int64  `json:"count"`
}

func (q *Queries) GetTransformBreakdown(ctx context.Context, userID pgtype.UUID) ([]GetTransformBreakdownRow, error) {
	rows, err := q.db.Query(ctx, getTransformBreakdown, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetTransformBreakdownRow
	for rows.Next() {
		var i GetTransformBreakdownRow
		if err := rows.Scan(&i.Type, &i.Count); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getUserUsageStats = `-- name: GetUserUsageStats :one
SELECT 
    COALESCE(COUNT(DISTINCT f.id) FILTER (WHERE f.deleted_at IS NULL), 0)::bigint as files_used,
    u.files_limit as files_limit,
    COALESCE(u.transformations_count, 0) as transforms_used,
    u.transformations_limit as transforms_limit,
    COALESCE(SUM(f.size_bytes) FILTER (WHERE f.deleted_at IS NULL), 0)::bigint as storage_used_bytes,
    u.subscription_tier as plan_name,
    COALESCE(u.subscription_period_end, u.transformations_reset_at) as plan_renews_at
FROM users u
LEFT JOIN files f ON f.user_id = u.id
WHERE u.id = $1 AND u.deleted_at IS NULL
GROUP BY u.id
`

type GetUserUsageStatsRow struct {
	FilesUsed        int64              `json:"files_used"`
	FilesLimit       int32              `json:"files_limit"`
	TransformsUsed   int32              `json:"transforms_used"`
	TransformsLimit  int32              `json:"transforms_limit"`
	StorageUsedBytes int64              `json:"storage_used_bytes"`
	PlanName         SubscriptionTier   `json:"plan_name"`
	PlanRenewsAt     pgtype.Timestamptz `json:"plan_renews_at"`
}

func (q *Queries) GetUserUsageStats(ctx context.Context, id pgtype.UUID) (GetUserUsageStatsRow, error) {
	row := q.db.QueryRow(ctx, getUserUsageStats, id)
	var i GetUserUsageStatsRow
	err := row.Scan(
		&i.FilesUsed,
		&i.FilesLimit,
		&i.TransformsUsed,
		&i.TransformsLimit,
		&i.StorageUsedBytes,
		&i.PlanName,
		&i.PlanRenewsAt,
	)
	return i, err
}

const getVideoProcessingStats = `-- name: GetVideoProcessingStats :one
SELECT
    COUNT(DISTINCT f.id)::bigint as total_video_files,
    COALESCE(SUM(f.size_bytes), 0)::bigint as total_video_bytes,
    COUNT(pj.id) FILTER (WHERE pj.job_type IN ('video_transcode', 'video_hls', 'video_thumbnail', 'video_watermark'))::bigint as total_video_jobs,
    COUNT(pj.id) FILTER (WHERE pj.job_type = 'video_transcode')::bigint as transcode_jobs,
    COUNT(pj.id) FILTER (WHERE pj.job_type = 'video_hls')::bigint as hls_jobs,
    COUNT(pj.id) FILTER (WHERE pj.job_type = 'video_thumbnail')::bigint as video_thumbnail_jobs,
    COALESCE(AVG(EXTRACT(EPOCH FROM (pj.completed_at - pj.started_at))) FILTER (WHERE pj.job_type = 'video_transcode'), 0)::float as avg_transcode_duration_seconds,
    COALESCE(SUM(EXTRACT(EPOCH FROM (pj.completed_at - pj.started_at))) FILTER (WHERE pj.job_type IN ('video_transcode', 'video_hls')), 0)::bigint as total_video_processing_seconds
FROM files f
LEFT JOIN processing_jobs pj ON pj.file_id = f.id AND pj.status = 'completed'
WHERE f.content_type LIKE 'video/%' AND f.deleted_at IS NULL
`

type GetVideoProcessingStatsRow struct {
	TotalVideoFiles             int64   `json:"total_video_files"`
	TotalVideoBytes             int64   `json:"total_video_bytes"`
	TotalVideoJobs              int64   `json:"total_video_jobs"`
	TranscodeJobs               int64   `json:"transcode_jobs"`
	HlsJobs                     int64   `json:"hls_jobs"`
	VideoThumbnailJobs          int64   `json:"video_thumbnail_jobs"`
	AvgTranscodeDurationSeconds float64 `json:"avg_transcode_duration_seconds"`
	TotalVideoProcessingSeconds int64   `json:"total_video_processing_seconds"`
}

// Get video processing statistics for cost forecasting
func (q *Queries) GetVideoProcessingStats(ctx context.Context) (GetVideoProcessingStatsRow, error) {
	row := q.db.QueryRow(ctx, getVideoProcessingStats)
	var i GetVideoProcessingStatsRow
	err := row.Scan(
		&i.TotalVideoFiles,
		&i.TotalVideoBytes,
		&i.TotalVideoJobs,
		&i.TranscodeJobs,
		&i.HlsJobs,
		&i.VideoThumbnailJobs,
		&i.AvgTranscodeDurationSeconds,
		&i.TotalVideoProcessingSeconds,
	)
	return i, err
}

const getVideoProcessingStatsByUser = `-- name: GetVideoProcessingStatsByUser :one
SELECT
    COUNT(DISTINCT f.id)::bigint as total_video_files,
    COALESCE(SUM(f.size_bytes), 0)::bigint as total_video_bytes,
    COUNT(pj.id) FILTER (WHERE pj.job_type IN ('video_transcode', 'video_hls', 'video_thumbnail', 'video_watermark'))::bigint as total_video_jobs,
    COUNT(pj.id) FILTER (WHERE pj.job_type = 'video_transcode')::bigint as transcode_jobs,
    COUNT(pj.id) FILTER (WHERE pj.job_type = 'video_hls')::bigint as hls_jobs,
    COALESCE(SUM(EXTRACT(EPOCH FROM (pj.completed_at - pj.started_at))) FILTER (WHERE pj.job_type IN ('video_transcode', 'video_hls')), 0)::bigint as total_video_processing_seconds
FROM files f
LEFT JOIN processing_jobs pj ON pj.file_id = f.id AND pj.status = 'completed'
WHERE f.user_id = $1 AND f.content_type LIKE 'video/%' AND f.deleted_at IS NULL
`

type GetVideoProcessingStatsByUserRow struct {
	TotalVideoFiles             int64 `json:"total_video_files"`
	TotalVideoBytes             int64 `json:"total_video_bytes"`
	TotalVideoJobs              int64 `json:"total_video_jobs"`
	TranscodeJobs               int64 `json:"transcode_jobs"`
	HlsJobs                     int64 `json:"hls_jobs"`
	TotalVideoProcessingSeconds int64 `json:"total_video_processing_seconds"`
}

// Get video processing statistics for a specific user
func (q *Queries) GetVideoProcessingStatsByUser(ctx context.Context, userID pgtype.UUID) (GetVideoProcessingStatsByUserRow, error) {
	row := q.db.QueryRow(ctx, getVideoProcessingStatsByUser, userID)
	var i GetVideoProcessingStatsByUserRow
	err := row.Scan(
		&i.TotalVideoFiles,
		&i.TotalVideoBytes,
		&i.TotalVideoJobs,
		&i.TranscodeJobs,
		&i.HlsJobs,
		&i.TotalVideoProcessingSeconds,
	)
	return i, err
}
